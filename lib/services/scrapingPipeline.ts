/**
 * Service de Pipeline de Scraping avec Filtrage Binaire Int√©gr√©
 *
 * Pipeline complet :
 * 1. Scraping Google Maps ‚Üí extraction des donn√©es brutes
 * 2. Validation qualit√© web ‚Üí filtrage binaire (garder seulement les probl√®mes)
 * 3. Import BDD ‚Üí uniquement les prospects avec opportunit√©s
 *
 * Objectif: Maximiser la pertinence commerciale des prospects
 */

import { ScrapingStatus } from '@prisma/client'
import { WebQualityFilter, WebsiteIssueResult } from './webQualityFilter'
import { db } from '../database'

export interface ScrapingConfig {
  metier: string          // "plombier", "√©lectricien", etc.
  villes: string[]        // ["Paris", "Lyon", etc.]
  maxProspects: number    // limite de prospects √† scraper
  userId: string          // utilisateur qui lance le scraping
}

export interface RawProspect {
  companyName: string
  fullName?: string
  email?: string
  phone?: string
  address?: string
  city?: string
  website?: string
  source: 'google_maps'
}

export interface ValidatedProspect extends RawProspect {
  websiteValidation?: WebsiteIssueResult
  shouldKeep: boolean
}

export interface ScrapingResult {
  scrapingJobId: string
  totalScraped: number
  prospectsWithIssues: number
  prospectsRejected: number
  imported: number
  rejectionReasons: {
    noWebsite: number
    modernSites: number
    validationErrors: number
  }
  duration: number // en millisecondes
}

export class ScrapingPipeline {
  private webQualityFilter: WebQualityFilter

  constructor() {
    this.webQualityFilter = new WebQualityFilter()
  }

  /**
   * Ex√©cute le pipeline complet de scraping avec filtrage binaire
   */
  async executePipeline(config: ScrapingConfig): Promise<ScrapingResult> {
    const startTime = Date.now()

    // 1. Cr√©er le job de scraping
    const scrapingJob = await this.createScrapingJob(config)

    try {
      return await this.executeProcessingOnly(scrapingJob.id, config)

    } catch (error) {
      console.error('Erreur pipeline scraping:', error)
      await this.failScrapingJob(scrapingJob.id, error)
      throw error
    }
  }

  /**
   * Ex√©cute seulement le traitement pour un job existant
   */
  async executeProcessingOnly(jobId: string, config: ScrapingConfig): Promise<ScrapingResult> {
    const startTime = Date.now()

    try {
      // 2. Scraping Google Maps
      await this.updateJobStatus(jobId, 'SCRAPING')
      const rawProspects = await this.scrapeGoogleMaps(config)

      console.log(`üìä ${rawProspects.length} prospects scrap√©s depuis Google Maps`)

      // 3. Filtrage par URL
      // NOTE: On garde TOUS les prospects, m√™me sans site web (car c'est une excellente opportunit√© commerciale)
      // La validation s'occupera de traiter le cas "pas de site" comme une opportunit√©.
      const prospectsToValidate = rawProspects

      console.log(`üåê ${prospectsToValidate.length} prospects √† valider (avec ou sans site)`)

      // 4. Validation qualit√© web avec filtrage binaire
      await this.updateJobStatus(jobId, 'VALIDATING')
      const validatedProspects = await this.validateProspectsQuality(prospectsToValidate)

      // 5. Filtrage binaire : garder seulement les probl√®mes
      await this.updateJobStatus(jobId, 'FILTERING')
      const prospectsToKeep = validatedProspects.filter(p => p.shouldKeep)
      const prospectsRejected = validatedProspects.filter(p => !p.shouldKeep)

      console.log(`‚úÖ ${prospectsToKeep.length} prospects gard√©s (avec probl√®mes web)`)
      console.log(`‚ùå ${prospectsRejected.length} prospects rejet√©s (sites corrects)`)

      // 6. Import en BDD
      await this.updateJobStatus(jobId, 'IMPORTING')
      const importedProspects = await this.importProspects(prospectsToKeep, config.userId)

      // 7. Calculer les statistiques de rejet
      const rejectionReasons = this.calculateRejectionStats(rawProspects, validatedProspects)

      // 8. Finaliser le job
      const duration = Date.now() - startTime
      await this.completeScrapingJob(jobId, {
        totalScraped: rawProspects.length,
        withIssues: prospectsToKeep.length,
        imported: importedProspects.length,
        rejected: prospectsRejected.length
      })

      // 9. Retourner les r√©sultats
      return {
        scrapingJobId: jobId,
        totalScraped: rawProspects.length,
        prospectsWithIssues: prospectsToKeep.length,
        prospectsRejected: prospectsRejected.length,
        imported: importedProspects.length,
        rejectionReasons,
        duration
      }

    } catch (error) {
      console.error('Erreur pipeline scraping:', error)
      await this.failScrapingJob(jobId, error)
      throw error
    }
  }

  /**
   * Scraping Google Maps - API externe (Scraping Dog)
   */
  private async scrapeGoogleMaps(config: ScrapingConfig): Promise<RawProspect[]> {
    console.log(`üîç Scraping Google Maps: ${config.metier} dans ${config.villes.join(', ')}`)

    const serpApiKey = process.env.SERPAPI_API_KEY

    if (!serpApiKey || serpApiKey === '') {
      console.log('‚ö†Ô∏è SerpAPI key manquante, utilisation des donn√©es mock√©es')
      return this.getMockData(config)
    }

    const allProspects: RawProspect[] = []

    // Scraper chaque ville
    for (const ville of config.villes) {
      try {
        const prospects = await this.scrapeCityWithSerpAPI(config.metier, ville, serpApiKey)
        allProspects.push(...prospects)

        // Respecter la limite max
        if (allProspects.length >= config.maxProspects) {
          break
        }
      } catch (error) {
        console.error(`Erreur scraping ${ville}:`, error)
        // En cas d'erreur API, utiliser les donn√©es mock√©es pour cette ville
        console.log(`‚ö†Ô∏è Fallback vers donn√©es mock√©es pour ${ville}`)
        const mockData = this.getMockDataForCity(config.metier, ville)
        allProspects.push(...mockData)
      }
    }

    // Limiter au nombre max demand√©
    return allProspects.slice(0, config.maxProspects)
  }

  private async scrapeCityWithSerpAPI(metier: string, ville: string, apiKey: string): Promise<RawProspect[]> {
    const { getJson } = require('serpapi')

    const params = {
      api_key: apiKey,
      engine: 'google_maps',
      q: `${metier} ${ville}`,
      type: 'search',
      num: 20
    }

    try {
      console.log(`üîç SerpAPI: Recherche ${metier} dans ${ville}`)
      const response = await getJson(params)

      if (!response.local_results || !Array.isArray(response.local_results)) {
        console.log(`‚ö†Ô∏è Aucun r√©sultat SerpAPI pour ${ville}`)
        return []
      }

      return response.local_results.map((item: any) => ({
        companyName: item.title || 'Entreprise sans nom',
        fullName: '',
        phone: item.phone || '',
        address: item.address || '',
        city: ville,
        website: item.website || '',
        source: 'google_maps' as const
      }))

    } catch (error) {
      console.error(`Erreur SerpAPI pour ${ville}:`, error)
      return []
    }
  }

  private getMockData(config: ScrapingConfig): RawProspect[] {
    const allMockProspects: RawProspect[] = []

    config.villes.forEach(ville => {
      const mockForCity = this.getMockDataForCity(config.metier, ville)
      allMockProspects.push(...mockForCity)
    })

    return allMockProspects.slice(0, config.maxProspects)
  }

  private getMockDataForCity(metier: string, ville: string): RawProspect[] {
    // Simulation de donn√©es Google Maps adapt√©es au m√©tier et ville
    return [
      {
        companyName: `${metier} ${ville} Pro`,
        fullName: `Jean ${ville}`,
        phone: '01.42.00.00.01',
        address: `123 rue de ${ville}`,
        city: ville,
        website: `https://${metier.toLowerCase()}-${ville.toLowerCase()}-ancien.fr`,
        source: 'google_maps'
      },
      {
        companyName: `Entreprise ${metier} ${ville}`,
        fullName: `Pierre ${ville}`,
        phone: '01.42.00.00.02',
        address: `456 avenue de ${ville}`,
        city: ville,
        website: `https://facebook.com/${metier.toLowerCase()}-${ville.toLowerCase()}`,
        source: 'google_maps'
      },
      {
        companyName: `${ville} ${metier} Services`,
        fullName: `Michel ${ville}`,
        phone: '01.42.00.00.03',
        address: `789 boulevard ${ville}`,
        city: ville,
        website: `https://modern-${metier.toLowerCase()}-${ville.toLowerCase()}.com`,
        source: 'google_maps'
      }
    ]
  }

  /**
   * Validation qualit√© web avec filtrage binaire
   */
  private async validateProspectsQuality(prospects: RawProspect[]): Promise<ValidatedProspect[]> {
    console.log(`üîç Validation qualit√© de ${prospects.length} sites web...`)

    const validatedProspects: ValidatedProspect[] = []

    // Valider par batches pour optimiser les performances
    for (let i = 0; i < prospects.length; i += 10) {
      const batch = prospects.slice(i, i + 10)

      const batchResults = await Promise.all(
        batch.map(async (prospect) => {
          if (!prospect.website) {
            return {
              ...prospect,
              websiteValidation: {
                hasWebsiteIssue: true,
                websiteIssueReason: 'Pas de site web',
                shouldKeepProspect: true
              },
              shouldKeep: true
            }
          }

          const validation = await this.webQualityFilter.validateWebsite(prospect.website)

          return {
            ...prospect,
            websiteValidation: validation,
            shouldKeep: validation.shouldKeepProspect
          }
        })
      )

      validatedProspects.push(...batchResults)

      // Log progress
      console.log(`‚úÖ Batch ${Math.floor(i / 10) + 1} valid√© (${Math.min(i + 10, prospects.length)}/${prospects.length})`)
    }

    return validatedProspects
  }

  /**
   * Import des prospects qualifi√©s en base de donn√©es
   */
  private async importProspects(prospects: ValidatedProspect[], userId: string): Promise<any[]> {
    console.log(`üíæ Import de ${prospects.length} prospects qualifi√©s en BDD...`)

    const importedProspects = await Promise.all(
      prospects.map(async (prospect) => {
        // V√©rifier les doublons avant import
        const existingProspect = await this.findDuplicate(prospect)

        if (existingProspect) {
          console.log(`‚ö†Ô∏è Doublon d√©tect√©: ${prospect.companyName}`)
          return null // Skip les doublons
        }

        // Cr√©er le nouveau prospect
        return await db.prospect.create({
          data: {
            companyName: prospect.companyName,
            fullName: prospect.fullName,
            email: prospect.email,
            phone: prospect.phone,
            address: prospect.address,
            city: prospect.city,
            website: prospect.website,
            hasWebsiteIssue: prospect.websiteValidation?.hasWebsiteIssue || false,
            websiteIssueReason: prospect.websiteValidation?.websiteIssueReason,
            lastWebsiteCheck: new Date(),
            status: 'PREMIER_APPEL',
            assignedTo: userId
          }
        })
      })
    )

    const successfulImports = importedProspects.filter(Boolean)
    console.log(`‚úÖ ${successfulImports.length} prospects import√©s avec succ√®s`)

    return successfulImports
  }

  /**
   * D√©tection de doublons simple
   */
  private async findDuplicate(prospect: ValidatedProspect): Promise<any> {
    // Rechercher par email si disponible
    if (prospect.email) {
      const byEmail = await db.prospect.findFirst({
        where: { email: prospect.email }
      })
      if (byEmail) return byEmail
    }

    // Rechercher par nom d'entreprise + ville
    if (prospect.companyName && prospect.city) {
      const byCompanyCity = await db.prospect.findFirst({
        where: {
          companyName: prospect.companyName,
          city: prospect.city
        }
      })
      if (byCompanyCity) return byCompanyCity
    }

    // Rechercher par t√©l√©phone si disponible
    if (prospect.phone) {
      const byPhone = await db.prospect.findFirst({
        where: { phone: prospect.phone }
      })
      if (byPhone) return byPhone
    }

    return null
  }

  /**
   * Calculer les statistiques de rejet
   */
  private calculateRejectionStats(rawProspects: RawProspect[], validatedProspects: ValidatedProspect[]) {
    const noWebsite = rawProspects.filter(p => !p.website || p.website.trim() === '').length
    const rejected = validatedProspects.filter(p => !p.shouldKeep)
    const modernSites = rejected.filter(p => p.websiteValidation?.websiteIssueReason === 'Site moderne et fonctionnel').length
    const validationErrors = rejected.filter(p => p.websiteValidation?.websiteIssueReason === 'Erreur de validation technique').length

    return {
      noWebsite,
      modernSites,
      validationErrors
    }
  }

  // ============================================================================
  // GESTION DES JOBS DE SCRAPING
  // ============================================================================

  private async createScrapingJob(config: ScrapingConfig) {
    return await db.scrapingJob.create({
      data: {
        status: 'PENDING',
        metier: config.metier,
        villes: config.villes,
        maxProspects: config.maxProspects,
        userId: config.userId
      }
    })
  }

  private async updateJobStatus(jobId: string, status: ScrapingStatus) {
    await db.scrapingJob.update({
      where: { id: jobId },
      data: { status }
    })
  }

  private async completeScrapingJob(jobId: string, results: {
    totalScraped: number
    withIssues: number
    imported: number
    rejected: number
  }) {
    await db.scrapingJob.update({
      where: { id: jobId },
      data: {
        status: 'COMPLETED',
        totalScraped: results.totalScraped,
        withIssues: results.withIssues,
        imported: results.imported,
        rejectedCount: results.rejected,
        completedAt: new Date()
      }
    })
  }

  private async failScrapingJob(jobId: string, error: any) {
    await db.scrapingJob.update({
      where: { id: jobId },
      data: {
        status: 'FAILED',
        errorMessage: error instanceof Error ? error.message : 'Erreur inconnue',
        completedAt: new Date()
      }
    })
  }
}